# -*- coding: utf-8 -*-
"""DataProcessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10qM0OfdopWdVlOiM_tXnbAIBr8xfK7c9
"""

# import data
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import requests
from bs4 import BeautifulSoup
import time
import pandas as pd
import os

columns = ['PROPERTY TYPE', 'ADDRESS', 'CITY', 'STATE OR PROVINCE', 'ZIP OR POSTAL CODE', 'PRICE', 'BEDS', 'BATHS',
           'SQUARE FEET', 'LOT SIZE', 'YEAR BUILT', 'DAYS ON MARKET', '$/SQUARE FEET', 'URL (SEE https://www.redfin.com/buy-a-home/comparative-market-analysis FOR INFO ON PRICING)',
           'LATITUDE', 'LONGITUDE']

def extract_image(url):
    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        # Find the first <img> tag
        first_img = soup.find('img')
        if first_img:
            return first_img.get('src')
    except Exception as e:
        print(f"Error fetching {url}: {e}")
    return None


def save_image(image_url, path):
    response = requests.get(image_url, stream=True)
    if response.status_code == 200:
        with open(path, 'wb') as file:
            # Write the image content in chunks
            for chunk in response.iter_content(1024):
                file.write(chunk)
        print(f"Image saved to {path}")
    else:
        print(f"Failed to retrieve image. Status code: {response.status_code}")

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'
}

full_df = pd.DataFrame(columns=['PROPERTY TYPE', 'ADDRESS', 'CITY', 'STATE OR PROVINCE', 'ZIP OR POSTAL CODE', 'PRICE', 'BEDS', 'BATHS',
           'SQUARE FEET', 'LOT SIZE', 'YEAR BUILT', 'DAYS ON MARKET', '$/SQUARE FEET', 'URL',
           'LATITUDE', 'LONGITUDE'])

full_df = pd.read_csv('/content/drive/MyDrive/CSC 374/Project/Data/all_listings.csv')

import re
def get_file_name(address):
  address = str(address)
  # Create a translation table once at the start of your script
  translation_table = str.maketrans({
      ' ': '_',
      '/': '_',
      '\\': '_',
      ':': '_',
      '*': '_',
      '?': '_',
      '"': '_',
      '<': '_',
      '>': '_',
      '|': '_'
  })
  # Then in your loop:
  address = address.translate(translation_table)
  address = re.sub(r'[^\w\-_.]', '_', address)
  return address

full_df['PHOTO_PATH'] = full_df['ADDRESS'].map(lambda x: f'/content/drive/MyDrive/CSC 374/Project/Data/final_project_images/{get_file_name(x)}.jpg')

import re

#iterate thru files in folder
for x in os.listdir('/content/drive/MyDrive/CSC 374/Project/Data/zipcode_csvs'):
  listing = pd.read_csv(f'/content/drive/MyDrive/CSC 374/Project/Data/zipcode_csvs/{x}')
  listing = listing[columns]
  listing.drop(0, inplace=True)
  listing.dropna(inplace=True)
  listing.drop_duplicates(keep='first', inplace=True)
  listing.rename(columns={'URL (SEE https://www.redfin.com/buy-a-home/comparative-market-analysis FOR INFO ON PRICING)': 'URL'}, inplace=True)
  listing.reset_index(drop=True, inplace=True)

  listing = listing.astype({"ADDRESS": "string", "PROPERTY TYPE": "string","CITY": "string", "STATE OR PROVINCE": "string"})
  print(listing.columns)

  # remove 0 beds or no urls
  listing.dropna(subset=['PRICE'], inplace=True)
  listing = listing[listing['BEDS'] > 0]
  listing = listing[listing['SQUARE FEET'] > 0]
  listing = listing[listing['URL'].notna()]
  redfin_urls = listing['URL'].tolist()
  if len(listing) != len(redfin_urls):
    print('error: listings and urls do not match in length')
  save_folder = f'/content/drive/MyDrive/CSC 374/Project/Data/final_project_images'

  # delete duplicates already in full csv
  full_df = pd.concat([full_df, listing])
  full_df.drop_duplicates(subset=['ADDRESS'], keep='first', inplace=True)

  # Create the 'PHOTO_PATH' column for the current listing DataFrame
  listing['PHOTO_PATH'] = listing['ADDRESS'].map(lambda p: f'/content/drive/MyDrive/CSC 374/Project/Data/final_project_images/{get_file_name(p)}.jpg')

  # Loop over the URLs and extract images
  idx = 0
  for url in redfin_urls:
      path = listing['PHOTO_PATH'].iloc[idx]
      if os.path.exists(path):
        print(f"{path}' already in folder")
        idx += 1
        continue
      image_url = extract_image(url)
      if image_url:
          save_image(image_url, path)
      else:
          print(f"No image found for {url}")
      idx += 1
      time.sleep(3)  # Delay to prevent overwhelming the server

full_df.to_csv('/content/drive/MyDrive/CSC 374/Project/Data/all_listings.csv', index=False)