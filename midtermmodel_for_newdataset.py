# -*- coding: utf-8 -*-
"""MidtermModel_for_newdataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZntGkWVIIAl9xl0pGpehCkNz8QoOIBPY
"""

from google.colab import drive
drive.mount('/content/drive')

import os
import re               # for get_file_name
import json             # NEW: to write meta.json
import joblib           # NEW: to dump scalers
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras import mixed_precision
from tensorflow.keras.layers import (
    Input, GlobalAveragePooling2D, Dense, Dropout, Concatenate,
    RandomFlip, RandomRotation, RandomZoom
)
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing import image as kpi
from tensorflow.keras.applications.resnet50 import preprocess_input
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
# -----------------------------------------------------------------------------
# 0. Setup mixed precision
# -----------------------------------------------------------------------------
policy = mixed_precision.Policy('mixed_float16')
mixed_precision.set_global_policy(policy)

# -----------------------------------------------------------------------------
# 1. Load & preprocess structured + image paths
# -----------------------------------------------------------------------------
df = pd.read_csv('/content/drive/MyDrive/CSC 374/Project/Data/all_listings.csv')

def get_file_name(address):
    address = str(address)
    translation_table = str.maketrans({
        ' ': '_','/': '_','\\': '_',':': '_','*': '_','?': '_',
        '"': '_','<': '_','>': '_','|': '_'
    })
    address = address.translate(translation_table)
    address = re.sub(r'[^\w\-_.]', '_', address)
    return address

df['PHOTO_PATH'] = df['ADDRESS'].map(
    lambda x: f'/content/drive/MyDrive/CSC 374/Project/Data/final_project_images/{get_file_name(x)}.jpg'
)
df = df[df['PHOTO_PATH'].apply(lambda p: tf.io.gfile.exists(p))].copy()
df = df.reset_index(drop=True)

image_paths = df['PHOTO_PATH'].astype(str).values

numeric_features = [
    'BEDS','BATHS','SQUARE FEET','LOT SIZE',
    'YEAR BUILT','DAYS ON MARKET',
    'LATITUDE','LONGITUDE'
]
categorical_features = ['PROPERTY TYPE','CITY']



print(f"Shape before processing: {df.shape}")

# 1a. scale numeric
scaler = StandardScaler()
df_num = pd.DataFrame(
    scaler.fit_transform(df[numeric_features]),
    columns=numeric_features
)
### NEW: save the structured scaler
os.makedirs("models", exist_ok=True)
joblib.dump(scaler, "models/struct_scaler.pkl")

# 1b. one-hot categorical
df_cat = pd.get_dummies(df[categorical_features], drop_first=True)

# 1c. combine
df_struct = pd.concat([df_num, df_cat], axis=1)
X_structured = df_struct.values

### NEW: write out meta.json for the app to know its columns
meta = {
    "numeric_features": numeric_features,
    #"categorical_features": categorical_features,
    "structured_columns": df_struct.columns.tolist()
}
with open("models/meta.json", "w") as f:
    json.dump(meta, f, indent=2)

# 1d. scale target
target_scaler = StandardScaler()
y_scaled = target_scaler.fit_transform(
    df['PRICE'].values.reshape(-1,1)
).flatten()
joblib.dump(target_scaler, "models/target_scaler.pkl")

assert X_structured.shape[0] == y_scaled.shape[0] == image_paths.shape[0]
print(f"Structured X shape: {X_structured.shape}, y shape: {y_scaled.shape}")

# 1e. train/val split
X_struct_train, X_struct_val, y_train, y_val, train_paths, val_paths = train_test_split(
    X_structured, y_scaled, image_paths, test_size=0.2, random_state=42
)

# -----------------------------------------------------------------------------
# 2. Image pipeline (unchanged)
# -----------------------------------------------------------------------------
AUTOTUNE = tf.data.AUTOTUNE
batch_size = 8

def load_and_preprocess_image(path):
    img = tf.io.read_file(path)
    img = tf.image.decode_jpeg(img, channels=3)
    img = tf.image.resize(img, [224,224])
    return preprocess_input(img)

def augment(img):
    img = tf.image.random_flip_left_right(img)
    img = tf.image.random_brightness(img, max_delta=0.1)
    img = tf.image.random_contrast(img, 0.9, 1.1)
    return img

def make_image_ds(paths, augment_flag=True):
    ds = tf.data.Dataset.from_tensor_slices(paths)
    ds = ds.map(load_and_preprocess_image, num_parallel_calls=AUTOTUNE)
    if augment_flag:
        ds = ds.map(augment, num_parallel_calls=AUTOTUNE)
    ds = ds.batch(batch_size).prefetch(AUTOTUNE)
    return ds

ds_images_train = make_image_ds(train_paths, augment_flag=True)
ds_images_val   = make_image_ds(val_paths, augment_flag=False)

# -----------------------------------------------------------------------------
# 3. Model definition
# -----------------------------------------------------------------------------
img_in = Input(shape=(224,224,3), name='image_input')
aug_layer = tf.keras.Sequential([
    RandomFlip('horizontal'),
    RandomRotation(0.1),
    RandomZoom(0.1)
], name='img_augment')
x = aug_layer(img_in)
base = tf.keras.applications.ResNet50(
    weights='imagenet',
    include_top=False,
    input_tensor=x
)
base.trainable = False
x = GlobalAveragePooling2D()(base.output)
x = Dense(128, activation='relu')(x)
x = Dropout(0.3)(x)

struct_in = Input(shape=(X_struct_train.shape[1],), name='structured_input')
y = Dense(32, activation='relu')(struct_in)
y = Dropout(0.2)(y)

combined = Concatenate()([x, y])
z = Dense(64, activation='relu')(combined)
z = Dropout(0.3)(z)
out = Dense(1, activation='linear', name='price_output')(z)

model = Model([img_in, struct_in], out)
model.compile(optimizer=Adam(1e-3), loss='mse', metrics=['mse','mae'])
model.summary()

# -----------------------------------------------------------------------------
# 4. Training + saving
# -----------------------------------------------------------------------------
ds_struct_train = tf.data.Dataset.from_tensor_slices(
    X_struct_train.astype('float32')
).batch(batch_size)
ds_struct_val   = tf.data.Dataset.from_tensor_slices(
    X_struct_val.astype('float32')
).batch(batch_size)

train_ds = tf.data.Dataset.zip((
    {'image_input': ds_images_train, 'structured_input': ds_struct_train},
    tf.data.Dataset.from_tensor_slices(y_train).batch(batch_size)
))
val_ds = tf.data.Dataset.zip((
    {'image_input': ds_images_val, 'structured_input': ds_struct_val},
    tf.data.Dataset.from_tensor_slices(y_val).batch(batch_size)
))

callbacks = [
    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),
    ModelCheckpoint(
        '/content/drive/MyDrive/CSC 374/Project/Data/results/md2_best_model.h5',
        monitor='val_loss', save_best_only=True
    )
]

history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=10,
    callbacks=callbacks
)

model.save("models/md2.keras", save_format="keras")
print("Saved md2.keras, struct_scaler.pkl, target_scaler.pkl & meta.json under models/")

# load model
model = tf.keras.models.load_model("models/md2.keras")

# -------------------------------
# 5. Evaluation & saving metrics
# -------------------------------
# get final training metrics from history
train_mse = history.history['loss'][-1]
train_mae = history.history['mae'][-1]
val_mse   = history.history['val_loss'][-1]
val_mae   = history.history['val_mae'][-1]


eval_loss, eval_mse, eval_mae = model.evaluate(val_ds, verbose=0)
print(f"Evaluate → Loss: {eval_loss:.4f}, MSE: {eval_mse:.4f}, MAE: {eval_mae:.4f}")


print(f"Training   → MSE: {train_mse:.4f}, MAE: {train_mae:.4f}")
print(f"Validation → MSE: {val_mse:.4f}, MAE: {val_mae:.4f}")



# write to results file
results_path = "/content/drive/MyDrive/CSC 374/Project/Data/results/md2_results.txt"
with open(results_path, "w") as f:
    f.write(f"Train MSE: {train_mse:.4f}, Train MAE: {train_mae:.4f}\n")
    f.write(f"Val   MSE: {val_mse:.4f}, Val   MAE: {val_mae:.4f}\n")
    f.write(f"Eval  MSE: {eval_mse:.4f}, Eval  MAE: {eval_mae:.4f}\n")

# Optionally, sample predictions
preds = model.predict(val_ds).flatten()
orig_preds = target_scaler.inverse_transform(preds.reshape(-1,1)).flatten()
print("Sample Predictions (orig scale):", orig_preds[:5])

from sklearn.linear_model import LinearRegression
from sklearn.dummy import DummyRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error

lin_reg = LinearRegression()
lin_reg.fit(X_struct_train, y_train)
y_lin_pred = lin_reg.predict(X_struct_val)

mse_lin = mean_squared_error(y_val, y_lin_pred)
mae_lin = mean_absolute_error(y_val, y_lin_pred)

print(f"Linear Regression Baseline - MSE: {mse_lin:.4f}, MAE: {mae_lin:.4f}")



# Assuming y_train and y_test are your training and testing targets in the original (or scaled) space.
dummy = DummyRegressor(strategy="mean")
dummy.fit(np.zeros_like(y_train).reshape(-1, 1), y_train)  # Dummy feature input since only target matters.
y_dummy_pred = dummy.predict(np.zeros_like(y_val).reshape(-1, 1))

mse_dummy = mean_squared_error(y_val, y_dummy_pred)
mae_dummy = mean_absolute_error(y_val, y_dummy_pred)

print(f"Dummy Baseline - MSE: {mse_dummy:.4f}, MAE: {mae_dummy:.4f}")